{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff403132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n",
    "# run_path_folder = Path(\"/Users/username/Downloads/cl_top_half_24_fps\")\n",
    "# run1_path = Path(\"/Users/username/Downloads/cl_top_half_24_fps_55_to_65_chunk1\")\n",
    "# run2_path = Path(\"/Users/username/Downloads/cl_top_half_24_fps_55_to_65_chunk2\")\n",
    "# run_paths = [run1_path, run2_path]\n",
    "run_paths = []\n",
    "# num_chunks = 16\n",
    "# for i in range(0, num_chunks):\n",
    "#     run_paths.append(Path(f\"/Users/username/Downloads/all_cl_24_fps_data/CL_TOP_HALF_24_FPS_CHUNK{i+1}.MP4\"))\n",
    "# run_paths\n",
    "\n",
    "for i in range(2, 3):\n",
    "    run_paths.append(Path(f\"/Users/username/Downloads/clipped_data/cl_clip_00{i}\"))\n",
    "run_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import dataclasses\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Tuple, cast\n",
    "\n",
    "import imageio.v3 as iio\n",
    "import numpy as np\n",
    "import numpy as onp\n",
    "import numpy.typing as onpt\n",
    "import skimage.transform\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "class Record3dLoader_Customized:\n",
    "    \"\"\"Helper for loading frames for Record3D captures.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir: Path, conf_threshold: float = 1.0, foreground_conf_threshold: float = 0.1, no_mask: bool = False, xyzw=True, init_conf=False):\n",
    "\n",
    "        # Read metadata.\n",
    "        intrinsics_path = data_dir / \"pred_intrinsics.txt\"\n",
    "        intrinsics = np.loadtxt(intrinsics_path)\n",
    "\n",
    "        self.K: onp.ndarray = np.array(intrinsics, np.float32).reshape(-1, 3, 3)\n",
    "        fps = 30\n",
    "\n",
    "        self.init_conf = init_conf\n",
    "\n",
    "        poses_path = data_dir / \"pred_traj.txt\"\n",
    "        poses = np.loadtxt(poses_path)\n",
    "        self.T_world_cameras: onp.ndarray = np.array(poses, np.float32)\n",
    "        self.T_world_cameras = np.concatenate(\n",
    "            [\n",
    "                # Convert TUM pose to SE3 pose\n",
    "                Rotation.from_quat(self.T_world_cameras[:, 4:]).as_matrix() if not xyzw\n",
    "                else Rotation.from_quat(np.concatenate([self.T_world_cameras[:, 5:], self.T_world_cameras[:, 4:5]], -1)).as_matrix(),\n",
    "                self.T_world_cameras[:, 1:4, None],\n",
    "            ],\n",
    "            -1,\n",
    "        )\n",
    "        self.T_world_cameras = self.T_world_cameras.astype(np.float32)\n",
    "\n",
    "        # Convert to homogeneous transformation matrices (ensure shape is (N, 4, 4))\n",
    "        num_frames = self.T_world_cameras.shape[0]\n",
    "        ones = np.tile(np.array([0, 0, 0, 1], dtype=np.float32), (num_frames, 1, 1))\n",
    "        self.T_world_cameras = np.concatenate([self.T_world_cameras, ones], axis=1)\n",
    "\n",
    "        self.fps = fps\n",
    "        self.conf_threshold = conf_threshold\n",
    "        self.foreground_conf_threshold = foreground_conf_threshold\n",
    "        self.no_mask = no_mask\n",
    "\n",
    "        # Read frames.\n",
    "        self.rgb_paths = sorted(data_dir.glob(\"frame_*.png\"), key=lambda p: int(p.stem.split(\"_\")[-1]))\n",
    "        self.depth_paths = sorted(data_dir.glob(\"frame_*.npy\"), key=lambda p: int(p.stem.split(\"_\")[-1]))\n",
    "        if init_conf:\n",
    "            self.init_conf_paths = sorted(data_dir.glob(\"init_conf_*.npy\"), key=lambda p: int(p.stem.split(\"_\")[-1]))\n",
    "        else:\n",
    "            self.init_conf_paths = []\n",
    "        self.conf_paths = sorted(data_dir.glob(\"conf_*.npy\"), key=lambda p: int(p.stem.split(\"_\")[-1]))\n",
    "        self.mask_paths = sorted(data_dir.glob(\"enlarged_dynamic_mask_*.png\"), key=lambda p: int(p.stem.split(\"_\")[-1]))\n",
    "\n",
    "        # Remove the last frame since it does not have a ground truth dynamic mask\n",
    "        self.rgb_paths = self.rgb_paths[:-1]\n",
    "\n",
    "        # Align all camera poses by the first frame\n",
    "        T0 = self.T_world_cameras[len(self.T_world_cameras) // 2]  # First camera pose (4x4 matrix)\n",
    "        T0_inv = np.linalg.inv(T0)    # Inverse of the first camera pose\n",
    "\n",
    "        # Apply T0_inv to all camera poses\n",
    "        self.T_world_cameras = np.matmul(T0_inv[np.newaxis, :, :], self.T_world_cameras)\n",
    "\n",
    "\n",
    "    def num_frames(self) -> int:\n",
    "        return len(self.rgb_paths)\n",
    "\n",
    "    def get_frame(self, index: int) -> Record3dFrame:\n",
    "\n",
    "        # Read depth.\n",
    "        depth = np.load(self.depth_paths[index])\n",
    "        depth: onp.NDArray[onp.float32] = depth\n",
    "        \n",
    "        # Check if conf file exists, otherwise initialize with ones\n",
    "        if len(self.conf_paths) == 0:\n",
    "            conf = np.ones_like(depth, dtype=onp.float32)\n",
    "        else:\n",
    "            conf_path = self.conf_paths[index]\n",
    "            if os.path.exists(conf_path):\n",
    "                conf = np.load(conf_path)\n",
    "                conf: onpt.NDArray[onp.float32] = conf\n",
    "                # Clip confidence to avoid negative values\n",
    "                conf = np.clip(conf, 0.0001, 99999)\n",
    "            else:\n",
    "                conf = np.ones_like(depth, dtype=onp.float32)\n",
    "\n",
    "        # Check if init conf file exists, otherwise initialize with ones\n",
    "        if len(self.init_conf_paths) == 0:  # If init conf is not available, use conf\n",
    "            init_conf = conf\n",
    "        else:\n",
    "            init_conf_path = self.init_conf_paths[index]\n",
    "            if os.path.exists(init_conf_path):\n",
    "                init_conf = np.load(init_conf_path)\n",
    "                init_conf: onpt.NDArray[onp.float32] = init_conf\n",
    "                # Clip confidence to avoid negative values\n",
    "                init_conf = np.clip(init_conf, 0.0001, 99999)\n",
    "            else:\n",
    "                init_conf = np.ones_like(depth, dtype=onp.float32)\n",
    "        \n",
    "        # Check if mask file exists, otherwise initialize with zeros\n",
    "        if len(self.mask_paths) == 0:\n",
    "            mask = np.ones_like(depth, dtype=onp.bool_)\n",
    "        else:\n",
    "            mask_path = self.mask_paths[index]\n",
    "            if os.path.exists(mask_path):\n",
    "                mask = iio.imread(mask_path) > 0\n",
    "                mask: onpt.NDArray[onp.bool_] = mask\n",
    "            else:\n",
    "                mask = np.ones_like(depth, dtype=onp.bool_)\n",
    "\n",
    "        if self.no_mask:\n",
    "            mask = np.ones_like(mask).astype(np.bool_)\n",
    "\n",
    "        # Read RGB.\n",
    "        rgb = iio.imread(self.rgb_paths[index])\n",
    "        # if 4 channels, remove the alpha channel\n",
    "        if rgb.shape[-1] == 4:\n",
    "            rgb = rgb[..., :3]\n",
    "\n",
    "        return Record3dFrame(\n",
    "            K=self.K[index],\n",
    "            rgb=rgb,\n",
    "            depth=depth,\n",
    "            mask=mask,\n",
    "            conf=conf,\n",
    "            init_conf=init_conf,\n",
    "            T_world_camera=self.T_world_cameras[index],\n",
    "            conf_threshold=self.conf_threshold,\n",
    "            foreground_conf_threshold=self.foreground_conf_threshold,\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Record3dFrame:\n",
    "    \"\"\"A single frame from a Record3D capture.\"\"\"\n",
    "\n",
    "    K: onpt.NDArray[onp.float32]\n",
    "    rgb: onpt.NDArray[onp.uint8]\n",
    "    depth: onpt.NDArray[onp.float32]\n",
    "    mask: onpt.NDArray[onp.bool_]\n",
    "    conf: onpt.NDArray[onp.float32]\n",
    "    init_conf: onpt.NDArray[onp.float32]\n",
    "    T_world_camera: onpt.NDArray[onp.float32]\n",
    "    conf_threshold: float = 1.0\n",
    "    foreground_conf_threshold: float = 0.1\n",
    "\n",
    "    def get_point_cloud(\n",
    "        self, downsample_factor: int = 1, bg_downsample_factor: int = 1,\n",
    "    ) -> Tuple[onpt.NDArray[onp.float32], onpt.NDArray[onp.uint8], onpt.NDArray[onp.float32], onpt.NDArray[onp.uint8]]:\n",
    "        rgb = self.rgb[::downsample_factor, ::downsample_factor]\n",
    "        depth = skimage.transform.resize(self.depth, rgb.shape[:2], order=0)\n",
    "        mask = cast(\n",
    "            onpt.NDArray[onp.bool_],\n",
    "            skimage.transform.resize(self.mask, rgb.shape[:2], order=0),\n",
    "        )\n",
    "        assert depth.shape == rgb.shape[:2]\n",
    "\n",
    "        K = self.K\n",
    "        T_world_camera = self.T_world_camera\n",
    "\n",
    "        img_wh = rgb.shape[:2][::-1]\n",
    "\n",
    "        grid = (\n",
    "            np.stack(np.meshgrid(np.arange(img_wh[0]), np.arange(img_wh[1])), 2) + 0.5\n",
    "        )\n",
    "        grid = grid * downsample_factor\n",
    "        conf_mask = self.conf > self.conf_threshold\n",
    "        if self.init_conf is not None:\n",
    "            fg_conf_mask = self.init_conf > self.foreground_conf_threshold\n",
    "        else:\n",
    "            fg_conf_mask = self.conf > self.foreground_conf_threshold\n",
    "        # reshape the conf mask to the shape of the depth\n",
    "        conf_mask = skimage.transform.resize(conf_mask, depth.shape, order=0)\n",
    "        fg_conf_mask = skimage.transform.resize(fg_conf_mask, depth.shape, order=0)\n",
    "\n",
    "        # Foreground points\n",
    "        homo_grid = np.pad(grid[fg_conf_mask & mask], ((0, 0), (0, 1)), constant_values=1)\n",
    "        local_dirs = np.einsum(\"ij,bj->bi\", np.linalg.inv(K), homo_grid)\n",
    "        dirs = np.einsum(\"ij,bj->bi\", T_world_camera[:3, :3], local_dirs)\n",
    "        points = (T_world_camera[:3, 3] + dirs * depth[fg_conf_mask & mask, None]).astype(np.float32)\n",
    "        point_colors = rgb[fg_conf_mask & mask]\n",
    "\n",
    "        # Background points\n",
    "        bg_homo_grid = np.pad(grid[conf_mask & ~mask], ((0, 0), (0, 1)), constant_values=1)\n",
    "        bg_local_dirs = np.einsum(\"ij,bj->bi\", np.linalg.inv(K), bg_homo_grid)\n",
    "        bg_dirs = np.einsum(\"ij,bj->bi\", T_world_camera[:3, :3], bg_local_dirs)\n",
    "        bg_points = (T_world_camera[:3, 3] + bg_dirs * depth[conf_mask & ~mask, None]).astype(np.float32)\n",
    "        bg_point_colors = rgb[conf_mask & ~mask]\n",
    "\n",
    "        if bg_downsample_factor > 1 and bg_points.shape[0] > 0:\n",
    "            indices = np.random.choice(\n",
    "                bg_points.shape[0],\n",
    "                size=bg_points.shape[0] // bg_downsample_factor,\n",
    "                replace=False\n",
    "            )\n",
    "            bg_points = bg_points[indices]\n",
    "            bg_point_colors = bg_point_colors[indices]\n",
    "        return points, point_colors, bg_points, bg_point_colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47458c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same parameters as viser src code defaults\n",
    "downsample_factor = 1\n",
    "max_frames = 100\n",
    "conf_threshold: float = 1\n",
    "foreground_conf_threshold: float = 0.1\n",
    "point_size: float = 0.001\n",
    "camera_frustum_scale: float = 0.02\n",
    "no_mask: bool = False\n",
    "xyzw: bool = True\n",
    "axes_scale: float = 0.25\n",
    "bg_downsample_factor: int = 1\n",
    "init_conf: bool = True\n",
    "cam_thickness: float = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39c51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = []\n",
    "for run_path in run_paths:\n",
    "    loader = Record3dLoader_Customized(\n",
    "        data_dir=run_path,\n",
    "        conf_threshold=conf_threshold,\n",
    "        foreground_conf_threshold=foreground_conf_threshold,\n",
    "        no_mask=no_mask,\n",
    "        xyzw=xyzw,\n",
    "        init_conf=init_conf\n",
    "    )\n",
    "    loaders.append(loader)\n",
    "    print(\"Number of frames in this loader:\", loader.num_frames())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1484ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loader1, loader2, loader3 = loaders\n",
    "# align_loader2_to_loader1(loader2, loader3)\n",
    "# align_loader2_to_loader1(loader1, loader2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de90f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaders = [Record]\n",
    "# loader1 = Record3dLoader_Customized(\n",
    "#         run1_path,\n",
    "#         conf_threshold=conf_threshold,\n",
    "#         foreground_conf_threshold=foreground_conf_threshold,\n",
    "#         no_mask=no_mask,\n",
    "#         xyzw=xyzw,\n",
    "#         init_conf=init_conf,\n",
    "#     )\n",
    "# loader2 = Record3dLoader_Customized(\n",
    "#         run2_path,\n",
    "#         conf_threshold=conf_threshold,\n",
    "#         foreground_conf_threshold=foreground_conf_threshold,\n",
    "#         no_mask=no_mask,\n",
    "#         xyzw=xyzw,\n",
    "#         init_conf=init_conf,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479d6452",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def align_loader2_to_loader1(loader1: Record3dLoader_Customized, loader2: Record3dLoader_Customized):\n",
    "    T1_last = loader1.T_world_cameras[-1]  # Last pose of loader1\n",
    "    T2_first = loader2.T_world_cameras[0]  # First pose of loader2\n",
    "\n",
    "    # Compute alignment transformation\n",
    "    T_align = T1_last @ np.linalg.inv(T2_first)\n",
    "\n",
    "    # Apply to all of loader2's poses\n",
    "    loader2.T_world_cameras = np.einsum('ij,njk->nik', T_align, loader2.T_world_cameras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f2d2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "\n",
    "def visualize_trajectories(loaders):\n",
    "    \"\"\"\n",
    "    Visualize camera‐center trajectories for a list of loader objects.\n",
    "    Each loader must have a `T_world_cameras` array of shape (N, 4, 4).\n",
    "    \"\"\"\n",
    "    # a simple palette—will wrap if you have more loaders than colors here\n",
    "    palette = [\n",
    "        [1.0, 0.0, 0.0],  # red\n",
    "        [0.0, 0.0, 1.0],  # blue\n",
    "        [0.0, 1.0, 0.0],  # green\n",
    "        [1.0, 1.0, 0.0],  # yellow\n",
    "        [1.0, 0.0, 1.0],  # magenta\n",
    "        [0.0, 1.0, 1.0],  # cyan\n",
    "    ]\n",
    "\n",
    "    geometries = []\n",
    "    for idx, loader in enumerate(loaders):\n",
    "        # Extract the 3D camera centers\n",
    "        traj = loader.T_world_cameras[:, :3, 3]\n",
    "        color = palette[idx % len(palette)]\n",
    "\n",
    "        # build point cloud\n",
    "        pcd = o3d.geometry.PointCloud()\n",
    "        pcd.points = o3d.utility.Vector3dVector(traj)\n",
    "        pcd.paint_uniform_color(color)\n",
    "        geometries.append(pcd)\n",
    "\n",
    "        # build line set to connect consecutive centers\n",
    "        lines = [[i, i + 1] for i in range(len(traj) - 1)]\n",
    "        colors = [color for _ in lines]\n",
    "        ls = o3d.geometry.LineSet()\n",
    "        ls.points = o3d.utility.Vector3dVector(traj)\n",
    "        ls.lines = o3d.utility.Vector2iVector(lines)\n",
    "        ls.colors = o3d.utility.Vector3dVector(colors)\n",
    "        geometries.append(ls)\n",
    "\n",
    "    o3d.visualization.draw_geometries(\n",
    "        geometries,\n",
    "        window_name=\"Aligned Trajectories\",\n",
    "        width=800,\n",
    "        height=600\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aeec7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in tqdm(range(1)):\n",
    "    # print(\"first pass\")\n",
    "    for i in range(len(loaders) - 1):\n",
    "        # print(f\"Aligning loader {i+1} to loader {i+2}\")\n",
    "        align_loader2_to_loader1(loaders[i], loaders[i + 1])\n",
    "    align_loader2_to_loader1(loaders[len(loaders) - 1], loaders[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f98ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d9899",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_trajectories(loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80556af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loader_to_pcd(loader):\n",
    "    num_frames = loader.num_frames()\n",
    "    all_positions = []\n",
    "    all_colors = []\n",
    "\n",
    "    bg_positions = []\n",
    "    bg_colors = []\n",
    "    for i in tqdm(range(num_frames)):\n",
    "        frame = loader.get_frame(i)\n",
    "        position, color, bg_position, bg_color = frame.get_point_cloud(downsample_factor, bg_downsample_factor)\n",
    "\n",
    "        all_positions.append(position)\n",
    "        all_colors.append(color)\n",
    "\n",
    "        bg_positions.append(bg_position)\n",
    "        bg_colors.append(bg_color)\n",
    "\n",
    "    all_positions = onp.concatenate(all_positions + bg_positions, axis=0)\n",
    "    all_colors = onp.concatenate(all_colors + bg_colors, axis=0)\n",
    "\n",
    "    final_point_cloud = {\n",
    "        \"points\": all_positions,\n",
    "        \"colors\": all_colors,\n",
    "    }\n",
    "    \n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    points = final_point_cloud[\"points\"].astype(np.float32)\n",
    "    colors = final_point_cloud[\"colors\"].astype(np.float32)\n",
    "    if colors.max() > 1.0:\n",
    "        colors /= 255.0\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    pcd.colors = o3d.utility.Vector3dVector(colors)\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae858c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds = [loader_to_pcd(loader) for loader in loaders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5a8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(pcds, window_name=\"Aligned Point Clouds\", width=2400, height=1720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2cb7b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017673e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a7d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(pcd, voxel_size):\n",
    "    # 1) Downsample\n",
    "    pcd_down = pcd\n",
    "    pcd_down = pcd.voxel_down_sample(voxel_size)\n",
    "\n",
    "    # 2) Estimate normals\n",
    "    pcd_down.estimate_normals(\n",
    "        search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "            radius=voxel_size * 2.0, max_nn=30))\n",
    "\n",
    "    # 3) Compute FPFH features\n",
    "    fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "        pcd_down,\n",
    "        o3d.geometry.KDTreeSearchParamHybrid(\n",
    "            radius=voxel_size * 5.0, max_nn=100))\n",
    "    return pcd_down, fpfh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77752b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pcd1)\n",
    "print(pcd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a8cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "voxel_size = 0.05\n",
    "ransac_dist = voxel_size * 2.0\n",
    "icp_dist    = voxel_size * 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d82344",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd1, fpfh1 = preprocess(pcd1, voxel_size)\n",
    "pcd2, fpfh2 = preprocess(pcd2, voxel_size)\n",
    "\n",
    "pcd1.estimate_normals(\n",
    "    search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "        radius=voxel_size * 2.0,  # same radius you used for FPFH normals\n",
    "        max_nn=30\n",
    "    )\n",
    ")\n",
    "pcd2.estimate_normals(\n",
    "    search_param=o3d.geometry.KDTreeSearchParamHybrid(\n",
    "        radius=voxel_size * 2.0,\n",
    "        max_nn=30\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a76dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pcd1)\n",
    "print(pcd2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bbfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_ransac = o3d.pipelines.registration.registration_ransac_based_on_feature_matching(\n",
    "    source=pcd2, target=pcd1,\n",
    "    source_feature=fpfh2,  target_feature=fpfh1,\n",
    "    mutual_filter=True,\n",
    "    max_correspondence_distance=ransac_dist,\n",
    "    estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPoint(False),\n",
    "    ransac_n=4,\n",
    "    checkers=[\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnEdgeLength(0.9),\n",
    "        o3d.pipelines.registration.CorrespondenceCheckerBasedOnDistance(ransac_dist)\n",
    "    ],\n",
    "    criteria=o3d.pipelines.registration.RANSACConvergenceCriteria(4000000, 500)\n",
    ")\n",
    "print(\"RANSAC fitness:\", result_ransac.fitness,\n",
    "      \"inlier_rmse:\", result_ransac.inlier_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9015cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_icp = o3d.pipelines.registration.registration_icp(\n",
    "    source=pcd2,                 # the cloud you want to align\n",
    "    target=pcd1,                 # the reference cloud\n",
    "    max_correspondence_distance=icp_dist,\n",
    "    init=result_ransac.transformation,\n",
    "    estimation_method=o3d.pipelines.registration.TransformationEstimationPointToPlane()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ad38b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ICP fitness: {result_icp.fitness:.4f}, RMSE: {result_icp.inlier_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b385a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd2.transform(result_icp.transformation)\n",
    "merged = pcd1 + pcd2\n",
    "o3d.io.write_point_cloud(\"merged_icp.ply\", merged)\n",
    "o3d.visualization.draw_geometries([merged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00638317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply it\n",
    "T = np.eye(4, dtype=np.float64)\n",
    "T[0, 3] = 1.0   # x shift\n",
    "T[1, 3] = 0.5   # y shift\n",
    "pcd2_transformed = pcd2.transform(T)\n",
    "\n",
    "# ——— Visualize to sanity‑check ———\n",
    "o3d.visualization.draw_geometries(\n",
    "    [pcd1, pcd2_transformed],\n",
    "    window_name=\"Debug: Hard‑coded transform\",\n",
    "    width=800, height=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533f973",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcdpcd2.transform(T)\n",
    "merged = pcd1 + pcd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.io.write_point_cloud(\"merged_ransac_only.ply\", merged)\n",
    "print(\"Saved merged point cloud to merged_ransac_only.ply\")\n",
    "\n",
    "o3d.visualization.draw_geometries(\n",
    "    [merged],\n",
    "    window_name=\"Merged (RANSAC only)\",\n",
    "    width=1024, height=768,\n",
    "    mesh_show_back_face=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7045f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
